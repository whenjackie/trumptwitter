{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from nltk.tokenize import TreebankWordTokenizer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "import numpy as np\n",
    "from collections import defaultdict\n",
    "from collections import Counter\n",
    "import math\n",
    "import time\n",
    "import csv\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# ['favorited', 'retweetCount', 'created', 'text', 'isRetweet', 'truncated', 'retweeted', 'replyToUID', \n",
    "# 'longitude', 'replyToSN', 'label', 'replyToSID', 'latitude', 'statusSource', 'favoriteCount', 'id', 'screenName']\n",
    "with open('all/train.csv', 'rb') as csvfile:\n",
    "    tweets = csv.DictReader(csvfile)\n",
    "    tweets = list(tweets)\n",
    "    l = len(tweets)\n",
    "    c_index = int(0.9*l)\n",
    "    train_data = [t['text'] for t in tweets[0:c_index]]\n",
    "    train_data2 = [(t['text'], t['statusSource']) for t in tweets[0:c_index]]\n",
    "    validate_content = [t['text'] for t in tweets[c_index:]]\n",
    "    validate_content2 = [(t['text'], t['statusSource']) for t in tweets[c_index:]]\n",
    "# with open('all/test.csv', 'rb') as csvfile:\n",
    "#     tweets = csv.DictReader(csvfile)\n",
    "#     tweets = list(tweets)\n",
    "#     print(tweets[0])\n",
    "#     test_content = [t['text'] for t in tweets]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "tfidf = TfidfVectorizer(max_df=0.95,min_df=0.02)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TfidfVectorizer(analyzer=u'word', binary=False, decode_error=u'strict',\n",
       "        dtype=<type 'numpy.int64'>, encoding=u'utf-8', input=u'content',\n",
       "        lowercase=True, max_df=0.95, max_features=None, min_df=0.02,\n",
       "        ngram_range=(1, 1), norm=u'l2', preprocessor=None, smooth_idf=True,\n",
       "        stop_words=None, strip_accents=None, sublinear_tf=False,\n",
       "        token_pattern=u'(?u)\\\\b\\\\w\\\\w+\\\\b', tokenizer=None, use_idf=True,\n",
       "        vocabulary=None)"
      ]
     },
     "execution_count": 128,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tfidf.fit(train_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "n_cols = len(tfidf.get_feature_names())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# tfidf.get_feature_names()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Attempt at Cosine Similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "word_splitter = re.compile(r\"\"\"\n",
    "    (\\w+)\n",
    "    \"\"\", re.VERBOSE)\n",
    "\n",
    "# sent_words = [word_splitter.findall(sent)\n",
    "#               for sent in content]\n",
    "\n",
    "sent_words = [sent.split() for sent in train_data]\n",
    "\n",
    "#list of words for each tweet\n",
    "sent_words_lower = [[w.lower() for w in sent]\n",
    "                    for sent in sent_words]\n",
    "\n",
    "terms=sorted(set([w for sent in sent_words_lower for w in sent]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# TF (term frequency) vectorization\n",
    "# We represent vectors in a \"sparse\" dictionary format.\n",
    "# All keys not present in the dictionary are assumed to be zeros.\n",
    "\n",
    "def doc_to_vec(term_list):\n",
    "    d = {}\n",
    "    for v in terms:\n",
    "        d[v] = term_list.count(v)\n",
    "    return d\n",
    "\n",
    "def query_to_vec(term_list):\n",
    "    d = {}\n",
    "    for v in terms:\n",
    "        d[v] = term_list.count(v)\n",
    "    return d\n",
    "\n",
    "def dot(d, q):\n",
    "    sum=0\n",
    "    for v in d:  # iterates through keys\n",
    "        sum += d[v] * q[v]\n",
    "    return sum\n",
    "\n",
    "def norm(d):\n",
    "    sum_sq = 0\n",
    "    for v in d:\n",
    "        sum_sq += d[v] * d[v]\n",
    "    return math.sqrt(sum_sq)\n",
    "\n",
    "def cos_measure(query_words, sentence):\n",
    "    A = query_to_vec(query_words)\n",
    "    B = doc_to_vec(sentence)\n",
    "    if norm(A)==0 or norm(B)==0: return 0\n",
    "    return float(dot(A, B)) / (norm(A) * norm(B))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 197,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from operator import itemgetter\n",
    "\n",
    "def run_search(query, similarity_measure):\n",
    "    query_words = [w.lower() for w in query.split()]\n",
    "    sent_scores = [(sent_words_lower[i], similarity_measure(query_words, sent_words_lower[i]), i)\n",
    "                   for i in range(len(sent_words_lower))]\n",
    "    #take top ten scoring sentences\n",
    "    sent_scores = sorted(sent_scores, key=itemgetter(1), reverse=True)\n",
    "    top_ten = []\n",
    "    sent_scores = [(sent, score, index) for sent, score, index in sent_scores[0:10] if score > 0]\n",
    "#     joined_sents = [(\" \".join(sent), score, i) for sent, score, i in sent_scores]\n",
    "    return sent_scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 198,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "x = run_search(validate_content[1], cos_measure)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "results = []\n",
    "for i in range(5):#len(validate_content)):\n",
    "    top_ten = run_search(validate_content[i], cos_measure)\n",
    "    a = 0\n",
    "    ip = 0\n",
    "    for sent, score, index in top_ten:\n",
    "        if 'Android' in train_data2[index][1]:\n",
    "            a += 1\n",
    "        elif 'iPhone' in train_data2[index][1]:\n",
    "            ip += 1\n",
    "    if a > ip:\n",
    "        results.append((i,'Android'))\n",
    "    else: \n",
    "        results.append((i, 'iPhone'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(0, 'Android'),\n",
       " (1, 'Android'),\n",
       " (2, 'Android'),\n",
       " (3, 'Android'),\n",
       " (4, 'Android')]"
      ]
     },
     "execution_count": 158,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#function to calculate accuracy\n",
    "results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4\n"
     ]
    }
   ],
   "source": [
    "correct = 0\n",
    "for r in results:\n",
    "    if 'Android' in validate_content2[r[0]][1] and 'Android' in r[1]:\n",
    "        correct += 1\n",
    "    elif 'iPhone' in validate_content2[r[0]][1] and 'iPhone' in r[1]:\n",
    "        correct += 1\n",
    "print(correct)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "IDF = {}\n",
    "DF = {}\n",
    "\n",
    "for t in terms:\n",
    "    DF[t] = len([1 for sent in sent_words_lower if t in sent])\n",
    "    IDF[t] = 1 / float(DF[t] + 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def doc_to_vec_df(term_list):\n",
    "    d = {}\n",
    "    for v in terms:\n",
    "        d[v] = term_list.count(v) * IDF[v]\n",
    "    return d\n",
    "\n",
    "def query_to_vec_df(term_list):\n",
    "    d = {}\n",
    "    for v in terms:\n",
    "        d[v] = term_list.count(v) * IDF[v]\n",
    "    return d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "inverted_index=defaultdict(list)\n",
    "for doc_idx, doc in enumerate(train_data): # by iterating this way we make sure that doc_ids will always appear sorted in the postings\n",
    "    for term in doc:\n",
    "        inverted_index[term].append(doc_idx)\n",
    "#         if doc_idx not in inverted_index[term]: # this way we have unique doc_ids      \n",
    "#             inverted_index[term].append(doc_idx) \n",
    "inverted_index_tf=defaultdict(list)\n",
    "for term in inverted_index:\n",
    "    postings=inverted_index[term]\n",
    "    if len(postings)>0:\n",
    "        lastdoc=-1\n",
    "        i=0\n",
    "        while i<len(postings):\n",
    "            tf=0\n",
    "            lastdoc=postings[i]\n",
    "            while i<len(postings) and lastdoc==postings[i]:\n",
    "                tf+=1\n",
    "                lastdoc=postings[i]\n",
    "                i+=1\n",
    "            inverted_index_tf[term].append((lastdoc,tf))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def merge_postings(term1,term2):\n",
    "    postings1=inverted_index[term1]\n",
    "    postings2=inverted_index[term2]\n",
    "    merged_posting=[]\n",
    "    i,j=0,0\n",
    "    while i<len(postings1) and j<len(postings2):\n",
    "        if postings1[i]==postings2[j]:\n",
    "            merged_posting.append(postings1[i])\n",
    "            i+=1\n",
    "            j+=1\n",
    "        elif postings1[i]<postings2[j]:\n",
    "            i+=1\n",
    "        else:\n",
    "            j+=1\n",
    "    return merged_posting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def docweight(queryterm, tf):\n",
    "    return tf*IDF[queryterm]\n",
    "\n",
    "doc_norms={}\n",
    "for doc_idx, doc in enumerate(train_data):\n",
    "    doc_norms[doc_idx]=norm(doc_to_vec(train_data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 214,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "## let's use the TF-IDF weighting \n",
    "def run_search_on_index(query):\n",
    "    query_words = [w.lower() for w in query.split()]\n",
    "    query_weights_dict={}  ## precomputed nonzero query  weights\n",
    "    large_vector=query_to_vec(query_words)\n",
    "    for t in large_vector:\n",
    "        if large_vector[t]>0:\n",
    "            query_weights_dict[t]=large_vector[t]\n",
    "    ## these are the score accumulators\n",
    "    ## they will accumulate accumulate TF-IDF weight products (w_iq * w_ij) \n",
    "    doc_scores=defaultdict(int)  \n",
    "    \n",
    "    ## since keys of our index are query words, we will iterate through those\n",
    "    for queryterm in query_words:\n",
    "        postings=inverted_index_tf[queryterm]\n",
    "        for doc_idx,tf in postings:   ## only touching docs that are involved in the query\n",
    "            if queryterm in query_weights_dict.keys():\n",
    "                weight_update=query_weights_dict[queryterm]*docweight(queryterm,tf)\n",
    "                doc_scores[doc_idx]+=weight_update  ## accumulate TF-IDF updates here \n",
    "            ##(at the end of the outer loop, for each document that has any of these terms\n",
    "            ##  we will have accumulated all TF-IDF weights that are needed to compute the \n",
    "            ## dot-product part of the cosine similarity score\n",
    "\n",
    "    for d in doc_scores:\n",
    "        if doc_norms[d] == 0 or norm(query_weights_dict) == 0:\n",
    "            doc_scores[d] = 0\n",
    "        else:\n",
    "            doc_scores[d]=doc_scores[d]/(float(doc_norms[d])*float(norm(query_weights_dict)))  \n",
    "        ## normalization part of the cosine similarity score\n",
    "        ## same ORDER if you remove the query norm (every document is divided by it)\n",
    "    \n",
    "    doc_idx_scores = sorted(doc_scores.items(), key=itemgetter(1), reverse=True)  ##further optimization possible\n",
    "    doc_scores = [(train_data[doc_idx], score, doc_idx)\n",
    "                   for doc_idx, score in doc_idx_scores\n",
    "                   if score > 0]\n",
    "\n",
    "    return doc_scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 215,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "results = []\n",
    "for i in range(len(validate_content)):\n",
    "    top_ten = run_search_on_index(validate_content[i])\n",
    "    a = 0\n",
    "    ip = 0\n",
    "    for sent, score, index in top_ten:\n",
    "        if 'Android' in train_data2[index][1]:\n",
    "            a += 1\n",
    "        elif 'iPhone' in train_data2[index][1]:\n",
    "            ip += 1\n",
    "    if a > ip:\n",
    "        results.append((i,'Android'))\n",
    "    else: \n",
    "        results.append((i, 'iPhone'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 216,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "51\n"
     ]
    }
   ],
   "source": [
    "correct = 0\n",
    "for r in results:\n",
    "    if 'Android' in validate_content2[r[0]][1] and 'Android' in r[1]:\n",
    "        correct += 1\n",
    "    elif 'iPhone' in validate_content2[r[0]][1] and 'iPhone' in r[1]:\n",
    "        correct += 1\n",
    "print(correct)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 217,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "109"
      ]
     },
     "execution_count": 217,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda root]",
   "language": "python",
   "name": "conda-root-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
